# -*- coding: utf-8 -*-
"""11MAY-Breast_Cancer_Final_Results.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A8i_v3YjEqLAVgnBjOiqOmEU4rAlL29k
"""

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive

import sys
sys.path.insert(0, 'drive/Ashoka forms/')
#sys.path.insert(1, 'drive')
PATH = 'drive/Ashoka forms/'

!python -m pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose

import keras
import keras.backend as K
import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Activation, Conv2D, Flatten, MaxPooling2D, Dropout, AveragePooling2D, BatchNormalization
from keras.utils import np_utils 
from keras import regularizers
from sklearn.cross_validation import  train_test_split
from sklearn import metrics
import cv2
import os
import glob
from keras.preprocessing.image import ImageDataGenerator,img_to_array, load_img
from keras.backend import tf as ktf
from time import time
from sklearn.preprocessing import StandardScaler
from keras.callbacks import ModelCheckpoint

# https://keras.io/
!pip install -q keras


# https://opencv.org/
!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python


labels =  pd.read_csv(PATH + 'Labels.csv') 

print(labels.columns)                                                     
d = {'M' : 0, 'B' : 1, 'N' : 2}
labels['diagnosis'] = labels['diagnosis'].map(d)

ROOT_DIR = 'drive/Ashoka forms/output1/'

# Put all images in the ROOT_DIR into the array named files

files = []
for file in glob.glob(ROOT_DIR + "*.pgm"):
    files.append(file)

# Create rows as many as we have at files array

row = len(files) * 3

width = 224
height = 224
num_class = 3
channel = 3
column = width * height * channel

X = []
y = []
index = 0

for file in files:
    if index % 100 == 0:
      print(index)
    img = cv2.imread(file)
    img = cv2.resize(img, (width, height))


    name = file.split('/')[-1]
    name = name.split('.')[0]
    
    
    X.append(img.ravel())
    y.append(labels[labels['Name']==name]['diagnosis'].values[0])
    
    
    index += 1

ROOT_DIR2 = 'drive/Ashoka forms/images-ia/'

files = []
for file in glob.glob(ROOT_DIR2 + "*.pgm"):
    files.append(file)

row = len(files) * 3

width = 224
height = 224
num_class = 3
channel = 3
column = width * height * channel

X2 = []
y2 = []
index = 0

for file in files:
    if index % 100 == 0:
      print(index)
    img = cv2.imread(file)
    img = cv2.resize(img, (width, height))

    name = file.split('/')[-1]
    name = name.split('.')[0]
    
    X2.append(img.ravel())
    y2.append(labels[labels['Name']+"_ia"==name]['diagnosis'].values[0])
    
    
    index += 1

ROOT_DIR3 = 'drive/Ashoka forms/images-ie/'

files = []
for file in glob.glob(ROOT_DIR3 + "*.pgm"):
    files.append(file)

# Create rows as many as we have at files array

row = len(files) * 3

width = 224
height = 224
num_class = 3
channel = 3
column = width * height * channel

X3 = []
y3 = []
index = 0

for file in files:
    if index % 100 == 0:
      print(index)
    img = cv2.imread(file)
    img = cv2.resize(img, (width, height))

    name = file.split('/')[-1]
    name = name.split('.')[0]
    
    X3.append(img.ravel())
    y3.append(labels[labels['Name']+"_ie"==name]['diagnosis'].values[0])
    
    
    index += 1

images_all= X+X2+X3

labels_all= y+y2+y3

images_all = np.array(images_all)
labels_all = np.array(labels_all)


print(images_all.shape)
print(labels_all.shape)
X = images_all
y = labels_all

print('Main Feature Matrix', images_all.shape)

trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.2, random_state=35, stratify=y)  # makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify

#Random Over Sampling: Creates balanced samples by randomly increasing the number of minority examples

!pip install imblearn 
from imblearn.over_sampling import RandomOverSampler
rus = RandomOverSampler(random_state=42)
X_res, y_res = rus.fit_sample(trainX, trainY)

trainX = X_res
trainY = y_res

trainX = trainX.reshape(trainX.shape[0], width, height, channel)
testX = testX.reshape(testX.shape[0], width, height, channel)

datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=40,
        width_shift_range=0.3,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip= True,              
        fill_mode='nearest')

datagen.fit(trainX)

testgen = ImageDataGenerator(
        rescale=1./255)

testgen.fit(testX)

# Convert labels to categorical one-hot encoding
trainY = keras.utils.to_categorical(trainY, num_classes=num_class)
testY = keras.utils.to_categorical(testY, num_classes=num_class)

datagen = datagen.flow(trainX,trainY)
testgen = testgen.flow(testX,testY)

trainY.shape

input_shape = (width, height, channel)

input_shape

#STARTING TO BUILD NEURAL NETWORK
model = Sequential()
model.add(Conv2D(128, kernel_size=(3, 3), padding='same',
                 activation='relu',
                 input_shape=input_shape))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(num_class, kernel_regularizer=regularizers.l2(0.1), activation='softmax'))

model.summary()

#checkpoint = ModelCheckpoint('drive/breast_cancer_dl/our_best_model_3.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(lr=0.1),  metrics=['accuracy'])

imsize=224
inputshape=(imsize,imsize,3)
inputshape

modelweight='imagenet'
from keras import applications
from keras.models import Model
from keras import optimizers

#TRANSFER LEARNING: is the method of using a pretrained repository, which is ImageNet here

model_transfer= applications.resnet50.ResNet50(weights=modelweight, include_top=False, input_shape=inputshape)
#model_transfer.summary()

for layer in model_transfer.layers:
  layer.trainable = False
  
x = model_transfer.output
x = Flatten()(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(1024, activation="relu")(x)
predictions = Dense(3, activation="softmax")(x)

model_final = Model(input = model_transfer.input, output = predictions)


model_final.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(lr=0.01), metrics=['accuracy'])

history= model_final.fit_generator(datagen, steps_per_epoch=1497, epochs=200, validation_data=testgen)  
# The number of steps_per_epoch equals to trainY.shape to enable that the kernel can process all pixels in each one epoch

#model.load_weights('drive/breast_cancer_dl/our_best_model_3.hdf5')
print(model.evaluate_generator(testgen))    #Test accuracy is low here hence we run 5 epochs before delivery to prove the generator works. 200 epochs' process takes a night. When we increase the number of epochs test accuracy directly increases to roundly 65%.
#model.save('drive/breast_cancer_dl/our_best_model_3.hdf5')

history.history.keys()
train_loss = history.history['loss']
train_acc = history.history['acc']
val_loss = history.history['val_loss'] 
val_acc = history.history['val_acc']

loss_dict = {'train_loss': train_loss, 'val_loss': val_loss}
acc_dict = {'train_acc': train_acc, 'val_acc': val_acc}

# %matplotlib inline
loss_df = pd.DataFrame(loss_dict)
acc_df = pd.DataFrame(acc_dict)

loss_df.plot()
acc_df.plot()

from sklearn.metrics import classification_report,confusion_matrix

Y_pred_prob = model.predict_generator(testgen)
print(Y_pred_prob)
Y_pred = np.argmax(Y_pred_prob,axis=1)
print(Y_pred)
print(len(Y_pred))

target_names = ['class 0(MALIGN)', 'class 1(BENIGN)', 'class 2(NORMAL)']
Y_true= np.argmax(testY,axis=1)
print(Y_true)
print(len(Y_true))

print(confusion_matrix(Y_true, Y_pred))

print(classification_report(Y_true, Y_pred ,target_names=target_names))

